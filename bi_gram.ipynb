{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk import ngrams\n",
        "import re\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "pfs6dQHCv3eJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"konbert-output-ad705d9f.csv\"  # Update with your actual file path\n",
        "df = pd.read_csv(file_path)\n",
        "df.drop(columns=['f'], inplace=True)\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to apply stemming\n",
        "def stem_text(text):\n",
        "    return \" \".join([stemmer.stem(word) for word in str(text).split()])\n",
        "\n",
        "# Function to apply lemmatization\n",
        "def lemmatize_text(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in str(text).split()])\n",
        "\n",
        "# Create a new DataFrame for transformed text\n",
        "df_transformed = df.copy()\n",
        "\n",
        "# Apply stemming first, then lemmatization (or reverse if needed)\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df_transformed[col] = df[col].apply(stem_text).apply(lemmatize_text)\n",
        "\n",
        "# Save the transformed DataFrame\n",
        "df_transformed.to_csv(\"transformed_data.csv\", index=False)\n",
        "\n",
        "print(\"Transformed data saved as 'transformed_data.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0VTgkrvvkxi",
        "outputId": "94a0b8fa-6dbd-4bc8-aefc-c9302dbe11e6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed data saved as 'transformed_data.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnN7Wc6zhT6M",
        "outputId": "960ee279-e125-46e6-95fb-088133d2a0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['arrang', 'articul', 'act', 'analyz', 'apprais', 'choos', 'associ', 'adapt', 'argu', 'assembl', 'cite', 'character', 'appli', 'break', 'categor', 'copi', 'back/back up', 'break down', 'assess', 'defin', 'clarifi', 'calcul', 'attach', 'collect', 'describ', 'classifi', 'chang', 'combin', 'draw', 'compar', 'compil', 'duplic', 'contrast', 'conclud', 'compos', 'identifi', 'convert', 'complet', 'consid', 'construct', 'indic', 'defend', 'comput', 'core', 'creat', 'label', 'demonstr', 'correl', 'counsel', 'design', 'list', 'critic', 'develop', 'match', 'differenti', 'debat', 'critiqu', 'devi', 'discus', 'discov', 'deduc', 'estim', 'memor', 'distinguish', 'dramat', 'detect', 'decid', 'evalu', 'name', 'employ', 'diagnos', 'explain', 'order', 'experi', 'diagram', 'facilit', 'outlin', 'express', 'formul', 'quot', 'extend', 'gener', 'determin', 'read', 'extrapol', 'discrimin', 'recal', 'illustr', 'dissect', 'hypothes', 'recit', 'give', 'implement', 'improv', 'recogn', 'give exampl', 'interpret', 'divid', 'integr', 'record', 'interview', 'grade', 'invent']\n",
            "['arrang', 'articul', 'act', 'analyz', 'apprai', 'choo', 'associ', 'adapt', 'argu', 'assembl', 'cite', 'charact', 'appli', 'break', 'categor', 'copi', 'back', 'break', 'assess', 'defin', 'clarifi', 'calcul', 'attach', 'collect', 'describ', 'classifi', 'chang', 'combin', 'draw', 'compar', 'compil', 'duplic', 'contrast', 'conclud', 'compo', 'identifi', 'convert', 'complet', 'consid', 'construct', 'indic', 'defend', 'comput', 'core', 'creat', 'label', 'demonstr', 'correl', 'counsel', 'design', 'list', 'critic', 'develop', 'match', 'differenti', 'debat', 'critiqu', 'devi', 'discu', 'discov', 'deduc', 'estim', 'memor', 'distinguish', 'dramat', 'detect', 'decid', 'evalu', 'name', 'employ', 'diagno', 'explain', 'order', 'experi', 'diagram', 'facilit', 'outlin', 'express', 'formul', 'quot', 'extend', 'gener', 'determin', 'read', 'extrapol', 'discrimin', 'recal', 'illustr', 'dissect', 'hypoth', 'recit', 'give', 'implement', 'improv', 'recogn', 'give', 'interpret', 'divid', 'integr', 'record', 'interview', 'grade', 'invent']\n",
            "      Question_ID                                           Question  \\\n",
            "0               1  Suppose prices of two goods are constant, expl...   \n",
            "1               1  Suppose prices of two goods are constant, expl...   \n",
            "2               2  Explain the concept of price leadership observ...   \n",
            "3               3  Define profit. Briefly explain how accounting ...   \n",
            "4               3  Define profit. Briefly explain how accounting ...   \n",
            "...           ...                                                ...   \n",
            "3832         2518  PEST and SWOT are popular strategy tools. Disc...   \n",
            "3833         2519  List the advantages and disadvantages of Publi...   \n",
            "3834         2520  Show your calculations for all THREE (3) optio...   \n",
            "3835         2520  Show your calculations for all THREE (3) optio...   \n",
            "3836         2522                                Define brand audit.   \n",
            "\n",
            "     Matched Action Verb   Category       BT Level  \n",
            "0                explain      Comp.  Comprehension  \n",
            "1                diagram   Analysis  Comprehension  \n",
            "2                explain      Comp.  Comprehension  \n",
            "3                  defin  Knowledge  Comprehension  \n",
            "4                explain      Comp.  Comprehension  \n",
            "...                  ...        ...            ...  \n",
            "3832               discu       None    Application  \n",
            "3833                list  Knowledge      Knowledge  \n",
            "3834              calcul      Appl.       Analysis  \n",
            "3835               discu       None       Analysis  \n",
            "3836               defin  Knowledge      Knowledge  \n",
            "\n",
            "[3837 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "# Initialize NLP tools\n",
        "stemmer = PorterStemmer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
        "\n",
        "# Load datasets\n",
        "#df1 = pd.read_csv('/content/transformed_data.csv')  # Dataset 1 (Action Verbs & Categories)\n",
        "df1 = df_transformed.copy()\n",
        "df2 = pd.read_csv('combined_dataset.csv')        # Dataset 2 (Questions & BT Levels)\n",
        "\n",
        "# Assign unique IDs to each question\n",
        "df2['Question_ID'] = range(1, len(df2) + 1)\n",
        "\n",
        "# Extract unique action verbs from df1\n",
        "action_verbs = pd.unique(df1[['Knowledge', 'Comp.', 'Appl.', 'Analysis', 'Eval.', 'Create']].values.ravel())\n",
        "action_verbs = [verb.strip().lower() for verb in action_verbs if pd.notna(verb)]\n",
        "print(action_verbs)\n",
        "action_verbs = [stemmer.stem(nlp(verb)[0].lemma_) for verb in action_verbs]\n",
        "print(action_verbs)\n",
        "\n",
        "# Function to generate bigrams\n",
        "def generate_bigrams(text):\n",
        "    words = re.findall(r'\\w+', text.lower())  # Tokenize words\n",
        "    words = [stemmer.stem(nlp(word)[0].lemma_) for word in words]  # Apply stemming & lemmatization\n",
        "    return [' '.join(bigram) for bigram in ngrams(words, 2)]\n",
        "\n",
        "# Match action verbs with questions\n",
        "matches = []\n",
        "for _, row in df2.iterrows():\n",
        "    question_id = row['Question_ID']\n",
        "    question_text = row['QUESTION']\n",
        "    question_bigrams = generate_bigrams(question_text)\n",
        "    bt_level = row['BT LEVEL']\n",
        "\n",
        "    for verb in action_verbs:\n",
        "        if any(verb in bigram for bigram in question_bigrams):\n",
        "            matches.append((question_id, question_text, verb, bt_level))\n",
        "\n",
        "# Create matches DataFrame\n",
        "matches_df = pd.DataFrame(matches, columns=['Question_ID', 'Question', 'Matched Action Verb', 'BT Level'])\n",
        "\n",
        "# Function to find category for a matched action verb\n",
        "def find_category(verb):\n",
        "    for column in df1.columns:  # Search in all category columns\n",
        "        column_values = df1[column].dropna().astype(str).str.lower().tolist()  # Ensure all values are strings\n",
        "        if verb in column_values:\n",
        "            return column  # Return the category name\n",
        "    return None\n",
        "\n",
        "\n",
        "# Map action verbs to categories\n",
        "matches_df['Category'] = matches_df['Matched Action Verb'].apply(find_category)\n",
        "\n",
        "# Final structured DataFrame\n",
        "final_df = matches_df[['Question_ID', 'Question', 'Matched Action Verb', 'Category', 'BT Level']]\n",
        "\n",
        "# Display final dataframe\n",
        "print(final_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.to_csv(\"final_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "_byKOt_GvI8S"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}